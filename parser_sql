Approach,Description,Pros,Cons,Best Use Case
"Current Approach (Optimized Multiprocessing in Kubernetes Pods)","Splits command list across Kubernetes pods, utilizes multiprocessing for parallel execution, and adjusts workers based on system resources.","‚úÖ Efficient workload distribution; ‚úÖ Dynamically adjusts workers based on available CPUs & memory; ‚úÖ Scales well with multiple pods; ‚úÖ Prevents memory overload by adjusting worker count; ‚úÖ Handles errors gracefully","‚ùå Requires a well-structured pod naming convention; ‚ùå Limited by how evenly commands are distributed across pods","Large-scale distributed command execution where system resources vary per node"
"Single-Pod Execution (No Distribution Across Pods)","Runs all commands within a single pod using multiprocessing without splitting the workload.","‚úÖ Simpler setup; ‚úÖ No dependency on Kubernetes pod indexing; ‚úÖ Still benefits from multiprocessing","‚ùå Doesn't leverage multiple pods efficiently; ‚ùå Can overload a single node if the command list is too large","Small-scale executions where only one instance of the container is running"
"Thread-Based Execution (Using concurrent.futures.ThreadPoolExecutor)","Uses threads instead of processes to run commands concurrently within a pod.","‚úÖ Lower memory overhead than multiprocessing; ‚úÖ Suitable for I/O-bound tasks (e.g., network requests)","‚ùå Python's Global Interpreter Lock (GIL) can limit CPU-bound task performance; ‚ùå Not efficient for CPU-intensive tasks like heavy computation","I/O-heavy workloads where threading benefits apply"
"Distributed Task Queue (Using Celery + Redis or RabbitMQ)","Uses a task queue like Celery to distribute commands dynamically across worker nodes, allowing better resource allocation.","‚úÖ More dynamic than static pod-based distribution; ‚úÖ Can scale elastically based on real-time load; ‚úÖ Provides better monitoring and retry mechanisms","‚ùå Requires a message broker setup (Redis/RabbitMQ); ‚ùå More complex than direct multiprocessing; ‚ùå Adds latency due to message queuing","Large-scale dynamic workloads where fault tolerance, retries, and scaling are important"
"Kubernetes Job Controller","Uses Kubernetes Jobs instead of static pod distribution. Each job processes a set of commands dynamically.","‚úÖ More resilient to failures than static pods; ‚úÖ Automatically manages pod scaling; ‚úÖ Ideal for batch processing","‚ùå More Kubernetes overhead than simple pods; ‚ùå Requires job scheduling configuration; ‚ùå Not as fast as direct multiprocessing","Large batch processing tasks where automatic retry and failover are needed"
"Best Approach Based on Use Case","If you need fast parallel execution with Kubernetes pods: ‚úÖ Current Approach (Multiprocessing with pod distribution); If running in a single container or local environment: ‚úÖ Single-Pod Multiprocessing; If dealing with network-bound (I/O) tasks: ‚úÖ Thread-Based Execution; If scaling dynamically based on task load: ‚úÖ Celery with Redis/RabbitMQ; If using Kubernetes and want automated job scheduling: ‚úÖ Kubernetes Job Controller","For your case, since you are running commands in Kubernetes pods, the current approach is the best as it efficiently utilizes multiprocessing while ensuring balanced workload distribution. However, if you need dynamic scaling, consider Celery with Redis or a Kubernetes Job Controller. üöÄ"



import os
import multiprocessing
import subprocess
import socket
import psutil

def run_command(command):
    """Runs a shell command with error handling"""
    try:
        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()

        if process.returncode == 0:
            print(f"Success: {command}")
        else:
            print(f"Failed: {command}\nError: {stderr.decode()}")
    except Exception as e:
        print(f"Error running command: {command}\n{str(e)}")

def get_available_cpus():
    """Returns available CPU count"""
    return os.cpu_count()

def get_available_memory():
    """Returns available memory in GB"""
    return psutil.virtual_memory().available / (1024 ** 3)  # Convert bytes to GB

if __name__ == "__main__":
    # Get pod index from hostname (Kubernetes sets this)
    try:
        pod_index = int(socket.gethostname().split("-")[-1])  # Extract number from pod name
    except (IndexError, ValueError):
        pod_index = 0  # Default to 0 if hostname format is unexpected

    # Read commands and distribute workload across replicas
    with open("/app/commands.txt", "r") as f:
        commands = [line.strip() for line in f.readlines() if line.strip()]

    total_replicas = max(1, int(os.getenv("TOTAL_REPLICAS", 1)))  # Ensure at least 1 replica
    num_commands = len(commands)
    
    if num_commands == 0:
        print("No commands found. Exiting.")
        exit(0)

    # Ensure at least one command per pod if commands < replicas
    chunk_size = max(1, num_commands // total_replicas)
    start = pod_index * chunk_size
    end = min(start + chunk_size, num_commands) if pod_index < total_replicas - 1 else num_commands

    pod_commands = commands[start:end]  # Assign part of the work to this pod

    print(f"Pod {pod_index} executing {len(pod_commands)} commands")

    # Get available resources
    available_memory = get_available_memory()
    available_cpus = get_available_cpus()

    # Set parallel workers: 2-3x CPU count, but adjust for memory
    num_workers = min(available_cpus * 3, 96)  # Limit workers to 96 as a max
    # Reduce workers if memory is a constraint
    if available_memory < 4:
        num_workers = available_cpus  # One process per CPU
    if available_memory < 2:
        num_workers = max(1, available_cpus // 2)  # Further reduce workers for low memory

    print(f"Running {len(pod_commands)} commands with {num_workers} parallel workers...\n")

    # Run the commands in parallel using multiprocessing
    with multiprocessing.Pool(num_workers) as pool:
        pool.map(run_command, pod_commands)  # Execute commands in parallel

    print(f"Pod {pod_index} execution complete")



import os
import multiprocessing
import subprocess
import socket
import psutil

def run_command(command):
    """Runs a shell command with error handling"""
    try:
        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()

        if process.returncode == 0:
            print(f"Success: {command}")
        else:
            print(f"Failed: {command}\nError: {stderr.decode()}")
    except Exception as e:
        print(f"Error running command: {command}\n{str(e)}")

def get_available_cpus():
    """Returns available CPU count"""
    return os.cpu_count()

def get_available_memory():
    """Returns available memory in GB"""
    return psutil.virtual_memory().available / (1024 ** 3)  # Convert bytes to GB

if __name__ == "__main__":
    # Get pod index from hostname (Kubernetes sets this)
    pod_index = int(socket.gethostname().split("-")[-1])  # Extract number from pod name

    # Read commands and distribute workload across replicas
    with open("/app/commands.txt", "r") as f:
        commands = [line.strip() for line in f.readlines()]

    total_replicas = int(os.getenv("TOTAL_REPLICAS", 10))  # Default 10 replicas
    chunk_size = len(commands) // total_replicas
    start = pod_index * chunk_size
    end = start + chunk_size if pod_index < total_replicas - 1 else len(commands)

    pod_commands = commands[start:end]  # Assign part of the work to this pod

    print(f"Pod {pod_index} executing {len(pod_commands)} commands")

    # Get available resources
    available_memory = get_available_memory()
    available_cpus = get_available_cpus()

    # Set parallel workers: 2-3x CPU count, but adjust for memory
    num_workers = min(available_cpus * 3, 96)  # Limit workers to 96 as a max
    # Reduce workers if memory is a constraint
    if available_memory < 4:
        num_workers = available_cpus  # One process per CPU

    print(f"Running {len(pod_commands)} commands with {num_workers} parallel workers...\n")

    # Run the commands in parallel using multiprocessing
    with multiprocessing.Pool(num_workers) as pool:
        pool.map(run_command, pod_commands)  # Execute commands in parallel

    print(f"Pod {pod_index} execution complete")



import os
import multiprocessing
import subprocess
import psutil

def get_available_cpus():
    """Returns available CPU count"""
    return os.cpu_count()

def get_available_memory():
    """Returns available memory in GB"""
    return psutil.virtual_memory().available / (1024 ** 3)  # GB

def run_command(command):
    """Runs a shell command"""
    try:
        process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()
        
        if process.returncode == 0:
            print(f"Success: {command}")
        else:
            print(f"Failed: {command}\nError: {stderr.decode()}")
    except Exception as e:
        print(f"Error running command: {command}\n{str(e)}")

if __name__ == "__main__":
    with open("commands.txt", "r") as f:
        commands = [line.strip() for line in f.readlines()]

    available_memory = get_available_memory()
    num_workers = min(10, get_available_cpus())  # Cap at 10 workers

    # Adjust workers if RAM is low
    if available_memory < 2:
        num_workers = 5  # Reduce workers if available memory is low

    print(f"Running {len(commands)} commands with {num_workers} parallel workers...\n")

    with multiprocessing.Pool(num_workers) as pool:
        pool.map(run_command, commands)  # Execute commands in parallel

    print("All commands executed!")













import os
import re
import pandas as pd


def extract_schema_table_column(sql, start_line):
    # Regular expression to find the table from the 'FROM' or 'JOIN' clause (schema.table or just table)
    table_regex = re.compile(
        r"(FROM|JOIN)\s+([a-zA-Z_][a-zA-Z0-9_]*)(?:\.\s*([a-zA-Z_][a-zA-Z0-9_]*))?(?:\s+AS\s+([a-zA-Z_][a-zA-Z0-9_]*))?",
        re.IGNORECASE)

    # Regular expression to find columns from the 'SELECT' clause
    columns_regex = re.compile(r"SELECT\s+(.*?)\s+(?:FROM|WHERE|GROUP BY|ORDER BY|LIMIT|HAVING)", re.IGNORECASE)

    schema_table_columns = []
    alias_to_table_map = {}

    print(f"Parsing SQL starting at line {start_line}")

    # Extract the schema and table (schema.optional.table)
    table_matches = table_regex.findall(sql)
    tables = []
    for match in table_matches:
        join_type = match[0]
        schema_name = match[2] if match[2] else "N/A"
        table_name = match[1]
        alias = match[3] if match[3] else "N/A"

        # If schema is not provided and the table is in the format schema.table, split it
        if schema_name == "N/A" and '.' in table_name:
            schema_name, table_name = table_name.split('.', 1)

        # Map alias to schema and table
        if alias != "N/A":
            alias_to_table_map[alias] = {"schema": schema_name, "table": table_name}

        tables.append({
            "Join Type": join_type,
            "Schema": schema_name if schema_name != "N/A" else "N/A",
            "Table": table_name if table_name != "N/A" else "N/A",
            "Alias": alias if alias != "N/A" else "N/A"
        })

        # Debugging the capture of table names
        print(f"Join Type: {join_type}, Schema: {schema_name}, Table: {table_name}, Alias: {alias}")

    # Extract columns, excluding any WHERE, GROUP BY, ORDER BY clauses
    column_match = columns_regex.search(sql)
    if column_match:
        # Extract columns (ignore the SQL clause details like WHERE, ORDER BY)
        columns_part = column_match.group(1).strip()

        # Split the columns by commas and clean extra spaces
        columns = [col.strip() for col in columns_part.split(',')]

        # For columns that reference aliases (e.g., e.first_name, d.department_name), map them to the correct table
        for idx, col in enumerate(columns):
            if '.' in col:
                alias, column = col.split('.', 1)
                if alias in alias_to_table_map:
                    schema, table = alias_to_table_map[alias]["schema"], alias_to_table_map[alias]["table"]
                    columns[idx] = f"{schema}.{table}.{column}"

        # Debugging the columns for each table
        print(f"Columns for Tables: {', '.join(columns)}")
    else:
        columns = ["N/A"]

    # Add schema, table, columns to the schema_table_columns list
    for table in tables:
        schema_table_columns.append({
            "Starting Line": start_line,
            "Schema": table['Schema'],
            "Table": table['Table'],
            "Columns": ", ".join(columns) if columns else "N/A"
        })

    return schema_table_columns


def add_static_columns(df, static_data_df):
    # Create an empty list to hold the updated rows
    updated_rows = []

    # Iterate through the rows of the DataFrame
    for index, row in df.iterrows():
        # Get the list of columns (split by commas)
        columns = row['Columns'].split(',')

        # Initialize the static values as "N/A" by default
        sensitivity_level = "N/A"
        critical_data_element = "N/A"
        pii = "N/A"
        financial_data = "N/A"
        health_data = "N/A"
        third_party_data = "N/A"
        regulatory_compliance = "N/A"

        # Check each column against the static data and update the static values
        for column in columns:
            column = column.strip()  # Remove any leading or trailing spaces
            print(f"Checking column: {column} in table: {row['Table']} and schema: {row['Schema']}")

            match = static_data_df[
                (static_data_df['Column Name'].str.lower() == column.lower()) &
                (static_data_df['Schema Name'].str.lower() == row['Schema'].lower()) &
                (static_data_df['Table Name'].str.lower() == row['Table'].lower())
                ]

            if not match.empty:
                # Extract the first match found (assuming one-to-one mapping for simplicity)
                sensitivity_level = match['Sensitivity Level'].values[0]
                critical_data_element = match['Critical Data Element'].values[0]
                pii = match['Personal Identifiable Information'].values[0]
                financial_data = match['Financial Data'].values[0]
                health_data = match['Health Data'].values[0]
                third_party_data = match['3rd Party Data'].values[0]
                regulatory_compliance = match['Regulatory and Compliance Data'].values[0]
                print(f"Match found for column: {column}")
                break  # Since it's a match, we can stop looking further
            else:
                print(f"No match found for column: {column}")

        # Add the updated row with the new static column values
        updated_row = row.copy()
        updated_row['Sensitivity Level'] = sensitivity_level
        updated_row['Critical Data Element'] = critical_data_element
        updated_row['Personal Identifiable Information'] = pii
        updated_row['Financial Data'] = financial_data
        updated_row['Health Data'] = health_data
        updated_row['3rd Party Data'] = third_party_data
        updated_row['Regulatory and Compliance Data'] = regulatory_compliance

        updated_rows.append(updated_row)

    # Create a new DataFrame with the updated rows
    updated_df = pd.DataFrame(updated_rows)

    return updated_df


def process_sql_files(input_folder, output_excel, static_data_df):
    all_schema_table_columns = []

    for file_name in os.listdir(input_folder):
        if file_name.endswith(".sql"):
            file_path = os.path.join(input_folder, file_name)
            with open(file_path, "r", encoding="utf-8") as f:
                sql_content = f.readlines()

            sql_statements = []
            current_statement = []
            for idx, line in enumerate(sql_content, start=1):
                line = line.strip()
                if line.startswith("--") or not line:
                    continue

                if ";" in line:
                    current_statement.append(line)
                    sql_statements.append((idx, " ".join(current_statement)))
                    current_statement = []
                else:
                    current_statement.append(line)

            for start_line, sql in sql_statements:
                try:
                    schema_table_columns = extract_schema_table_column(sql, start_line)
                    all_schema_table_columns.extend(schema_table_columns)
                except Exception as e:
                    print(f"Error processing SQL statement: {sql}\n{e}")

    # Output to DataFrame before adding static columns
    df = pd.DataFrame(all_schema_table_columns)

    # Add static columns based on reference data
    updated_df = add_static_columns(df, static_data_df)

    # Write final output to Excel
    updated_df.to_excel(output_excel, index=False)
    print(f"Output written to {output_excel}")


if __name__ == "__main__":
    input_folder = "input"  # Folder containing the SQL files
    output_excel = "output.xlsx"  # Name of the Excel file to store the output

    # Sample static data for matching with SQL columns
    static_data = {
        'Table Name': ['employees', 'departments', 'employees', 'employees'],
        'Schema Name': ['N/A', 'N/A', 'N/A', 'N/A'],
        'Column Name': ['e.employee_id', 'd.department_id', 'e.salary', 'e.first_name'],
        'Sensitivity Level': ['High', 'Low', 'High', 'Low'],
        'Critical Data Element': ['Y', 'N', 'Y', 'N'],
        'Personal Identifiable Information': ['Y', 'N', 'Y', 'N'],
        'Financial Data': ['Y', 'N', 'Y', 'N'],
        'Health Data': ['N', 'N', 'N', 'N'],
        '3rd Party Data': ['Y', 'N', 'N', 'N'],
        'Regulatory and Compliance Data': ['Y', 'N', 'Y', 'Y']
    }

    static_data_df = pd.DataFrame(static_data)

    process_sql_files(input_folder, output_excel, static_data_df)
